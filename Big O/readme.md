# Big O

Big O notation helps us understand how long an algorithm will take to run or how much memory it will need as the amount of data it handles grows.

# O(n)

Signifies that the execution time of the algorithm grows linearly in proportion to the size of the input data (n).

# O(1)

O(1) aka constant time, signifies that the execution time of an algorithm remains constant regardless of the input size.

# O(n^2)

Indicates that the algorithm's execution time grows quadratically with the size of the input data (represented by n).
Example: Imagine you have a box of items and want to compare each item with every other item to find specific pairs. As the number of items (n) increases, the number of comparisons (n^2) grows much faster.

# O(log n)

O(log n) time complexity refers to an algorithm's runtime that grows logarithmically with the size of the input (represented by n). In simpler terms, as the input size increases, the time it takes for the algorithm to run increase slowly.
